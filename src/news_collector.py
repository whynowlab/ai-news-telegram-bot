"""
News Collector - RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
"""
import feedparser
import hashlib
import json
import os
from datetime import datetime, timedelta, timezone
from dataclasses import dataclass, asdict
from typing import List, Optional
from pathlib import Path

from config import NEWS_SOURCES, NewsSource, CACHE_HOURS


@dataclass
class NewsItem:
    """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì•„ì´í…œ"""
    id: str                    # ê³ ìœ  ID (URL í•´ì‹œ)
    title: str                 # ì œëª©
    link: str                  # ì›ë¬¸ ë§í¬
    summary: str               # ì›ë¬¸ ìš”ì•½/ì„¤ëª…
    source_name: str           # ì†ŒìŠ¤ ì´ë¦„
    source_trust: int          # ì†ŒìŠ¤ ì‹ ë¢°ë„
    category: str              # ì¹´í…Œê³ ë¦¬
    published: Optional[str]   # ë°œí–‰ì¼
    collected_at: str          # ìˆ˜ì§‘ ì‹œê°„
    
    def to_dict(self):
        return asdict(self)


class NewsCollector:
    def __init__(self, cache_dir: str = "data"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.seen_file = self.cache_dir / "seen_news.json"
        self.seen_ids = self._load_seen_ids()
    
    def _load_seen_ids(self) -> dict:
        """ì´ë¯¸ ì²˜ë¦¬í•œ ë‰´ìŠ¤ ID ë¡œë“œ"""
        if self.seen_file.exists():
            try:
                with open(self.seen_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save_seen_ids(self):
        """ì²˜ë¦¬í•œ ë‰´ìŠ¤ ID ì €ì¥"""
        # ì˜¤ë˜ëœ í•­ëª© ì •ë¦¬ (48ì‹œê°„ ì´ìƒ)
        cutoff = datetime.now(timezone.utc) - timedelta(hours=CACHE_HOURS)
        cutoff_str = cutoff.isoformat()
        
        cleaned = {
            k: v for k, v in self.seen_ids.items()
            if v.get('seen_at', '') > cutoff_str
        }
        self.seen_ids = cleaned
        
        with open(self.seen_file, 'w', encoding='utf-8') as f:
            json.dump(self.seen_ids, f, ensure_ascii=False, indent=2)
    
    def _generate_id(self, url: str) -> str:
        """URL ê¸°ë°˜ ê³ ìœ  ID ìƒì„±"""
        return hashlib.md5(url.encode()).hexdigest()[:12]
    
    def _parse_published_date(self, entry) -> Optional[str]:
        """ë°œí–‰ì¼ íŒŒì‹±"""
        for attr in ['published', 'updated', 'created']:
            if hasattr(entry, attr) and getattr(entry, attr):
                return getattr(entry, attr)
        return None
    
    def _get_summary(self, entry) -> str:
        """ìš”ì•½ ì¶”ì¶œ"""
        # summary ë˜ëŠ” description í•„ë“œ í™•ì¸
        if hasattr(entry, 'summary') and entry.summary:
            return self._clean_html(entry.summary)[:500]
        if hasattr(entry, 'description') and entry.description:
            return self._clean_html(entry.description)[:500]
        return ""
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        import re
        clean = re.sub(r'<[^>]+>', '', text)
        clean = re.sub(r'\s+', ' ', clean).strip()
        return clean
    
    def collect_from_source(self, source: NewsSource) -> List[NewsItem]:
        """ë‹¨ì¼ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        items = []
        
        try:
            # User-Agent ì„¤ì • (Reddit ë“±ì—ì„œ í•„ìš”)
            feedparser.USER_AGENT = "AI-News-Bot/1.0 (Personal Use)"
            
            feed = feedparser.parse(source.url)
            
            if feed.bozo and not feed.entries:
                print(f"âš ï¸ {source.name}: í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨")
                return items
            
            for entry in feed.entries[:15]:  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 15ê°œ
                link = entry.get('link', '')
                if not link:
                    continue
                
                news_id = self._generate_id(link)
                
                # ì´ë¯¸ ì²˜ë¦¬í•œ ë‰´ìŠ¤ ìŠ¤í‚µ
                if news_id in self.seen_ids:
                    continue
                
                item = NewsItem(
                    id=news_id,
                    title=entry.get('title', 'No Title'),
                    link=link,
                    summary=self._get_summary(entry),
                    source_name=source.name,
                    source_trust=source.base_trust,
                    category=source.category,
                    published=self._parse_published_date(entry),
                    collected_at=datetime.now(timezone.utc).isoformat()
                )
                
                items.append(item)
            
            print(f"âœ… {source.name}: {len(items)}ê°œ ìƒˆ ë‰´ìŠ¤")
            
        except Exception as e:
            print(f"âŒ {source.name}: ìˆ˜ì§‘ ì‹¤íŒ¨ - {e}")
        
        return items
    
    def collect_all(self) -> List[NewsItem]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_items = []
        
        print(f"\nğŸ“¡ {len(NEWS_SOURCES)}ê°œ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...\n")
        
        for source in NEWS_SOURCES:
            items = self.collect_from_source(source)
            all_items.extend(items)
        
        print(f"\nğŸ“Š ì´ {len(all_items)}ê°œ ìƒˆ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ\n")
        
        return all_items
    
    def mark_as_seen(self, news_id: str):
        """ë‰´ìŠ¤ë¥¼ ì²˜ë¦¬ë¨ìœ¼ë¡œ í‘œì‹œ"""
        self.seen_ids[news_id] = {
            'seen_at': datetime.now(timezone.utc).isoformat()
        }
        self._save_seen_ids()
    
    def mark_multiple_as_seen(self, news_ids: List[str]):
        """ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ ì²˜ë¦¬ë¨ìœ¼ë¡œ í‘œì‹œ"""
        now = datetime.now(timezone.utc).isoformat()
        for news_id in news_ids:
            self.seen_ids[news_id] = {'seen_at': now}
        self._save_seen_ids()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    collector = NewsCollector(cache_dir="data")
    items = collector.collect_all()
    
    for item in items[:5]:
        print(f"\n{'='*50}")
        print(f"ğŸ“° {item.title}")
        print(f"ğŸ”— {item.link}")
        print(f"ğŸ“ {item.summary[:100]}...")
        print(f"â­ ì‹ ë¢°ë„: {item.source_trust}/10 | ì†ŒìŠ¤: {item.source_name}")
